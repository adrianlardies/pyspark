{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones de fecha y hora**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['JAVA_HOME'] = \"C:/Program Files/Java/jdk-11\"\n",
    "os.environ['PYSPARK_PYTHON'] = \"C:/Users/usr/anaconda3/envs/pyspark_env/python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"C:/Users/usr/anaconda3/envs/pyspark_env/python.exe\"\n",
    "os.environ['HADOOP_HOME'] = \"C:/hadoop-3.4.0\"\n",
    "os.environ['HADOOP_COMMON_LIB_NATIVE_DIR'] = \"C:/hadoop-3.4.0/lib/native\"\n",
    "os.environ['PATH'] += os.pathsep + \"C:/hadoop-3.4.0/bin\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part1 = spark.read.parquet('./data/convertir/part-00000-6b84f509-ffb8-43d4-820e-866fb80c0d08-c000.snappy.parquet')\n",
    "data_part2 = spark.read.parquet('./data/convertir/part-00001-6b84f509-ffb8-43d4-820e-866fb80c0d08-c000.snappy.parquet')\n",
    "\n",
    "data = data_part1.union(data_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- date_str: string (nullable = true)\n",
      " |-- ts_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------------+\n",
      "|date      |timestamp              |date_str  |ts_str          |\n",
      "+----------+-----------------------+----------+----------------+\n",
      "|2021-01-01|2021-01-01 20:10:50.723|01-01-2021|18-08-2021 46:58|\n",
      "+----------+-----------------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos a convertir de formato string a formato date y timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.select(\n",
    "    to_date(col('date')).alias('date1'),\n",
    "    to_timestamp(col('timestamp')).alias('ts1'),\n",
    "    to_date(col('date_str'), 'dd-MM-yyyy').alias('date2'),\n",
    "    to_timestamp(col('ts_str'), 'dd-MM-yyyy mm:ss').alias('ts2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+-------------------+\n",
      "|date1     |ts1                    |date2     |ts2                |\n",
      "+----------+-----------------------+----------+-------------------+\n",
      "|2021-01-01|2021-01-01 20:10:50.723|2021-01-01|2021-08-18 00:46:58|\n",
      "+----------+-----------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date1: date (nullable = true)\n",
      " |-- ts1: timestamp (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- ts2: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ahora vamos a darle formato a una fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|date_format(date1, dd-MM-yyyy)|\n",
      "+------------------------------+\n",
      "|                    01-01-2021|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.select(\n",
    "    date_format(col('date1'), 'dd-MM-yyyy')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cálculos con fechas y horas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('./data/calculo/calculo.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+-------------------+\n",
      "|nombre|fecha_ingreso|fecha_salida|       baja_sistema|\n",
      "+------+-------------+------------+-------------------+\n",
      "|  Jose|   2021-01-01|  2021-11-14|2021-10-14 15:35:59|\n",
      "|Mayara|   2021-02-06|  2021-11-25|2021-11-25 10:35:55|\n",
      "+------+-------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, last_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+--------------+\n",
      "|nombre|dias|      meses|ultimo_dia_mes|\n",
      "+------+----+-----------+--------------+\n",
      "|  Jose| 317|10.41935484|    2021-11-30|\n",
      "|Mayara| 292| 9.61290323|    2021-11-30|\n",
      "+------+----+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('nombre'),\n",
    "    datediff(col('fecha_salida'), col('fecha_ingreso')).alias('dias'),\n",
    "    months_between(col('fecha_salida'), col('fecha_ingreso')).alias('meses'),\n",
    "    last_day(col('fecha_salida')).alias('ultimo_dia_mes')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sumar y restar fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------+-----------+\n",
      "|nombre|fecha_ingreso|mas_14_dias|menos_1_dia|\n",
      "+------+-------------+-----------+-----------+\n",
      "|  Jose|   2021-01-01| 2021-01-15| 2020-12-31|\n",
      "|Mayara|   2021-02-06| 2021-02-20| 2021-02-05|\n",
      "+------+-------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('nombre'),\n",
    "    col('fecha_ingreso'),\n",
    "    date_add(col('fecha_ingreso'), 14).alias('mas_14_dias'),\n",
    "    date_sub(col('fecha_ingreso'), 1).alias('menos_1_dia')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extraer valores específicos de una columna date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, dayofyear, hour, minute, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|       baja_sistema|year(baja_sistema)|month(baja_sistema)|dayofmonth(baja_sistema)|dayofyear(baja_sistema)|hour(baja_sistema)|minute(baja_sistema)|second(baja_sistema)|\n",
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|2021-10-14 15:35:59|              2021|                 10|                      14|                    287|                15|                  35|                  59|\n",
      "|2021-11-25 10:35:55|              2021|                 11|                      25|                    329|                10|                  35|                  55|\n",
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('baja_sistema'),\n",
    "    year(col('baja_sistema')),\n",
    "    month(col('baja_sistema')),\n",
    "    dayofmonth(col('baja_sistema')),\n",
    "    dayofyear(col('baja_sistema')),\n",
    "    hour(col('baja_sistema')),\n",
    "    minute(col('baja_sistema')),\n",
    "    second(col('baja_sistema'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones para trabajo con strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('./data/data/data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformaciones string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| nombre|\n",
      "+-------+\n",
      "| Spark |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tiene espacios en blanco al principio y al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ltrim, rtrim, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "| ltrim| rtrim| trim|\n",
      "+------+------+-----+\n",
      "|Spark | Spark|Spark|\n",
      "+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    ltrim(col('nombre')).alias('ltrim'), # Elimina los espacios a la izquierda.\n",
    "    rtrim(col('nombre')).alias('rtrim'), # Elimina los espacios a la derecha.\n",
    "    trim(col('nombre')).alias('trim') # Elimina los espacios a la izquierda y derecha.\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rellenar string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lpad, rpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    lpad|    rpad|\n",
      "+--------+--------+\n",
      "|---Spark|Spark===|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    trim(col('nombre')).alias('trim'),\n",
    ").select(\n",
    "    lpad(col('trim'), 8, '-').alias('lpad'), # Agrega guiones a la izquierda.\n",
    "    rpad(col('trim'), 8, '=').alias('rpad') # Agrega igual al final.\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([('Spark', 'es', 'maravilloso')], ['sujeto', 'verbo', 'adjetivo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+\n",
      "|sujeto|verbo|   adjetivo|\n",
      "+------+-----+-----------+\n",
      "| Spark|   es|maravilloso|\n",
      "+------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concatenación, mayúsculas, minúsculas y reverso de string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, lower, upper, initcap, reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               frase|           minuscula|           mayuscula|             initcap|             reversa|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Spark es maravilloso|spark es maravilloso|SPARK ES MARAVILLOSO|Spark Es Maravilloso|osollivaram se krapS|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\n",
    "    concat_ws(' ', col('sujeto'), col('verbo'), col('adjetivo')).alias('frase')\n",
    ").select(\n",
    "    col('frase'),\n",
    "    lower(col('frase')).alias('minuscula'),\n",
    "    upper(col('frase')).alias('mayuscula'),\n",
    "    initcap(col('frase')).alias('initcap'),\n",
    "    reverse(col('frase')).alias('reversa')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame([(' voy a casa por mis llaves',)], ['frase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|frase                     |\n",
      "+--------------------------+\n",
      "| voy a casa por mis llaves|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|nueva_frase             |\n",
      "+------------------------+\n",
      "| ir a casa ir mis llaves|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\n",
    "    regexp_replace(col('frase'), 'voy|por', 'ir').alias('nueva_frase') # Reemplaza \"voy\" o \"por\" por ir.\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones para el trabajo con colecciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('./data/data_collections/parquet/part-00000-96f39196-ef97-4a14-926e-b24a86c2e32d-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------+\n",
      "|dia  |tareas                                      |\n",
      "+-----+--------------------------------------------+\n",
      "|lunes|[hacer la tarea, buscar agua, lavar el auto]|\n",
      "+-----+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dia: string (nullable = true)\n",
      " |-- tareas: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size, sort_array, array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------+-----------+\n",
      "|tamaño|array_ordenado                              |buscar_agua|\n",
      "+------+--------------------------------------------+-----------+\n",
      "|3     |[buscar agua, hacer la tarea, lavar el auto]|true       |\n",
      "+------+--------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    size(col('tareas')).alias('tamaño'),\n",
    "    sort_array(col('tareas')).alias('array_ordenado'),\n",
    "    array_contains(col('tareas'), 'buscar agua').alias('buscar_agua')\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|dia  |tareas        |\n",
      "+-----+--------------+\n",
      "|lunes|hacer la tarea|\n",
      "|lunes|buscar agua   |\n",
      "|lunes|lavar el auto |\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    col('dia'),\n",
    "    explode(col('tareas')).alias('tareas')\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Formato JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_str = spark.read.parquet('./data/data_collections/JSON/part-00000-9f0e2b16-0b50-4212-9b83-654e1b8dd137-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|tareas_str                                                                 |\n",
      "+---------------------------------------------------------------------------+\n",
      "|{\"dia\": \"lunes\",\"tareas\": [\"hacer la tarea\",\"buscar agua\",\"lavar el auto\"]}|\n",
      "+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df_str.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tareas_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df_str.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convertimos en estructura Spark, para ello debemos describir su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_json = StructType(\n",
    "    [\n",
    "        StructField('dia', StringType(), True),\n",
    "        StructField('tareas', ArrayType(StringType()), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = json_df_str.select(\n",
    "    from_json(col('tareas_str'), schema_json).alias('por_hacer')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- por_hacer: struct (nullable = true)\n",
      " |    |-- dia: string (nullable = true)\n",
      " |    |-- tareas: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------------------------------+--------------+\n",
      "|por_hacer.dia|por_hacer.tareas                            |primer_tarea  |\n",
      "+-------------+--------------------------------------------+--------------+\n",
      "|lunes        |[hacer la tarea, buscar agua, lavar el auto]|hacer la tarea|\n",
      "+-------------+--------------------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(\n",
    "    col('por_hacer').getItem('dia'),\n",
    "    col('por_hacer').getItem('tareas'),\n",
    "    col('por_hacer').getItem('tareas').getItem(0).alias('primer_tarea')\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|to_json(por_hacer)                                                       |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"dia\":\"lunes\",\"tareas\":[\"hacer la tarea\",\"buscar agua\",\"lavar el auto\"]}|\n",
      "+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(\n",
    "    to_json(col('por_hacer'))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones when, coalesce y lit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('./data/data/when_coalesce_lit.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|nombre|pago|\n",
      "+------+----+\n",
      "|  Jose|   1|\n",
      "| Julia|   2|\n",
      "| Katia|   1|\n",
      "|  null|   3|\n",
      "|  Raul|   3|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|nombre|       pago|\n",
      "+------+-----------+\n",
      "|  Jose|     pagado|\n",
      "| Julia|  sin pagar|\n",
      "| Katia|     pagado|\n",
      "|  null|sin iniciar|\n",
      "|  Raul|sin iniciar|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    col('nombre'),\n",
    "    when(col('pago') == 1, 'pagado').when(col('pago') == 2, 'sin pagar').otherwise('sin iniciar').alias('pago')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con coalesce toma uno o más valores de columna y devuelve el primero que no es nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|    nombre|\n",
      "+----------+\n",
      "|      Jose|\n",
      "|     Julia|\n",
      "|     Katia|\n",
      "|sin nombre|\n",
      "|      Raul|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    coalesce(col('nombre'), lit('sin nombre')).alias('nombre') \n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones definidas por el usuario (UDF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creamos una función para calcular el cubo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_cubo(n):\n",
    "    return n * n * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Registramos nuestra función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indicamos el nombre que le queremos dar, el nombre de la función que escribimos previamente y el tipo de dato que va a retornar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.f_cubo(n)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register('cubo', f_cubo, LongType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testeamos la función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creamos una lista a través de una vista temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1,10).createOrReplaceTempView('df_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|cubo|\n",
      "+---+----+\n",
      "|  1|   1|\n",
      "|  2|   8|\n",
      "|  3|  27|\n",
      "|  4|  64|\n",
      "|  5| 125|\n",
      "|  6| 216|\n",
      "|  7| 343|\n",
      "|  8| 512|\n",
      "|  9| 729|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT id, cubo(id) AS cubo FROM df_temp').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otra alternativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bienvenida(nombre):\n",
    "    return ('Hola {}'.format(nombre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos a dar la bienvenida a cada parámetro x que tome la función lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bienvenida_udf = udf(lambda x: bienvenida(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nombre = spark.createDataFrame([('Jose',), ('Julia',)], ['nombre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|nombre|\n",
      "+------+\n",
      "|  Jose|\n",
      "| Julia|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nombre.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|nombre|bien_nombre|\n",
      "+------+-----------+\n",
      "|  Jose|  Hola Jose|\n",
      "| Julia| Hola Julia|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nombre.select(\n",
    "    col('nombre'),\n",
    "    bienvenida_udf(col('nombre')).alias('bien_nombre')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3ª alternativa para crear un udf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def mayuscula(s):\n",
    "    return s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|nombre|may_nombre|\n",
      "+------+----------+\n",
      "|  Jose|      JOSE|\n",
      "| Julia|     JULIA|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nombre.select(\n",
    "    col('nombre'),\n",
    "    mayuscula(col('nombre')).alias('may_nombre')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas udf (udf vectorizadas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tenemos que especificar que el parámetro que recibe la función es de tipo pandas Series y el tipo de parámetro que va a devolver (->)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubo_pandas(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubo_udf = pandas_udf(cubo_pandas, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     8\n",
      "2    27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(cubo_pandas(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|cubo_pandas|\n",
      "+---+-----------+\n",
      "|  0|          0|\n",
      "|  1|          1|\n",
      "|  2|          8|\n",
      "|  3|         27|\n",
      "|  4|         64|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('id'), # La columna 'id' nos la crear por defecto.\n",
    "    cubo_udf(col('id')).alias('cubo_pandas')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones de ventana**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('./data/funciones_ventana.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+\n",
      "| nombre|edad|departamento|evaluacion|\n",
      "+-------+----+------------+----------+\n",
      "| Lazaro|  45|      letras|        98|\n",
      "|   Raul|  24|  matemática|        76|\n",
      "|  Maria|  34|  matemática|        27|\n",
      "|   Jose|  30|     química|        78|\n",
      "| Susana|  51|     química|        98|\n",
      "|   Juan|  44|      letras|        89|\n",
      "|  Julia|  55|      letras|        92|\n",
      "|  Kadir|  38|arquitectura|        39|\n",
      "| Lilian|  23|arquitectura|        94|\n",
      "|   Rosa|  26|      letras|        91|\n",
      "|   Aian|  50|  matemática|        73|\n",
      "|Yaneisy|  29|      letras|        89|\n",
      "|Enrique|  40|     química|        92|\n",
      "|    Jon|  25|arquitectura|        78|\n",
      "|  Luisa|  39|arquitectura|        94|\n",
      "+-------+----+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trabajadores con evaluación más alta por departamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, row_number, rank, dense_rank, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Especificación de ventana: vamos a neceitar particionar los datos y ordenarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creamos grupos de trabajadores por departamentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy('departamento').orderBy(desc('evaluacion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- row_number: número de filas secuencial comenzando desde 1 hasta el resultado de cada partición de ventana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----------+\n",
      "| nombre|edad|departamento|evaluacion|row_number|\n",
      "+-------+----+------------+----------+----------+\n",
      "| Lilian|  23|arquitectura|        94|         1|\n",
      "|  Luisa|  39|arquitectura|        94|         2|\n",
      "| Lazaro|  45|      letras|        98|         1|\n",
      "|  Julia|  55|      letras|        92|         2|\n",
      "|   Raul|  24|  matemática|        76|         1|\n",
      "|   Aian|  50|  matemática|        73|         2|\n",
      "| Susana|  51|     química|        98|         1|\n",
      "|Enrique|  40|     química|        92|         2|\n",
      "+-------+----+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('row_number', row_number().over(windowSpec)).filter(col('row_number').isin(1, 2)).show() # Agrega una columna al DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rank: rango al resultado dentro de una partición de ventana, esta función deja huecos en el rango cuando hay empates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----+\n",
      "| nombre|edad|departamento|evaluacion|rank|\n",
      "+-------+----+------------+----------+----+\n",
      "| Lilian|  23|arquitectura|        94|   1|\n",
      "|  Luisa|  39|arquitectura|        94|   1|\n",
      "|    Jon|  25|arquitectura|        78|   3|\n",
      "|  Kadir|  38|arquitectura|        39|   4|\n",
      "| Lazaro|  45|      letras|        98|   1|\n",
      "|  Julia|  55|      letras|        92|   2|\n",
      "|   Rosa|  26|      letras|        91|   3|\n",
      "|   Juan|  44|      letras|        89|   4|\n",
      "|Yaneisy|  29|      letras|        89|   4|\n",
      "|   Raul|  24|  matemática|        76|   1|\n",
      "|   Aian|  50|  matemática|        73|   2|\n",
      "|  Maria|  34|  matemática|        27|   3|\n",
      "| Susana|  51|     química|        98|   1|\n",
      "|Enrique|  40|     química|        92|   2|\n",
      "|   Jose|  30|     química|        78|   3|\n",
      "+-------+----+------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('rank', rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si no queremos dejar huecos trabajamos con la función dense_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----------+\n",
      "| nombre|edad|departamento|evaluacion|dense_rank|\n",
      "+-------+----+------------+----------+----------+\n",
      "| Lilian|  23|arquitectura|        94|         1|\n",
      "|  Luisa|  39|arquitectura|        94|         1|\n",
      "|    Jon|  25|arquitectura|        78|         2|\n",
      "|  Kadir|  38|arquitectura|        39|         3|\n",
      "| Lazaro|  45|      letras|        98|         1|\n",
      "|  Julia|  55|      letras|        92|         2|\n",
      "|   Rosa|  26|      letras|        91|         3|\n",
      "|   Juan|  44|      letras|        89|         4|\n",
      "|Yaneisy|  29|      letras|        89|         4|\n",
      "|   Raul|  24|  matemática|        76|         1|\n",
      "|   Aian|  50|  matemática|        73|         2|\n",
      "|  Maria|  34|  matemática|        27|         3|\n",
      "| Susana|  51|     química|        98|         1|\n",
      "|Enrique|  40|     química|        92|         2|\n",
      "|   Jose|  30|     química|        78|         3|\n",
      "+-------+----+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('dense_rank', dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agregaciones con funciones de ventana, no es necesario utilizar la clausula orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpecAgg = Window.partitionBy('departamento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+---+---+------------------+----------+\n",
      "| nombre|edad|departamento|evaluacion|min|max|               avg|row_number|\n",
      "+-------+----+------------+----------+---+---+------------------+----------+\n",
      "| Lilian|  23|arquitectura|        94| 39| 94|             76.25|         1|\n",
      "|  Luisa|  39|arquitectura|        94| 39| 94|             76.25|         2|\n",
      "|    Jon|  25|arquitectura|        78| 39| 94|             76.25|         3|\n",
      "|  Kadir|  38|arquitectura|        39| 39| 94|             76.25|         4|\n",
      "| Lazaro|  45|      letras|        98| 89| 98|              91.8|         1|\n",
      "|  Julia|  55|      letras|        92| 89| 98|              91.8|         2|\n",
      "|   Rosa|  26|      letras|        91| 89| 98|              91.8|         3|\n",
      "|   Juan|  44|      letras|        89| 89| 98|              91.8|         4|\n",
      "|Yaneisy|  29|      letras|        89| 89| 98|              91.8|         5|\n",
      "|   Raul|  24|  matemática|        76| 27| 76|58.666666666666664|         1|\n",
      "|   Aian|  50|  matemática|        73| 27| 76|58.666666666666664|         2|\n",
      "|  Maria|  34|  matemática|        27| 27| 76|58.666666666666664|         3|\n",
      "| Susana|  51|     química|        98| 78| 98| 89.33333333333333|         1|\n",
      "|Enrique|  40|     química|        92| 78| 98| 89.33333333333333|         2|\n",
      "|   Jose|  30|     química|        78| 78| 98| 89.33333333333333|         3|\n",
      "+-------+----+------------+----------+---+---+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.withColumn('min', min('evaluacion').over(windowSpecAgg))\n",
    ".withColumn('max', max('evaluacion').over(windowSpecAgg))\n",
    ".withColumn('avg', avg('evaluacion').over(windowSpecAgg))\n",
    ".withColumn('row_number', row_number().over(windowSpec)) # row_number() si necesita una especificación de ventana con clausula orderBy.\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Catalyst Optimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimizador de consultas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- explain() para mostrar los planes lógicos y físicos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('./data/data/vuelos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|YEAR|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|SCHEDULED_DEPARTURE|DEPARTURE_TIME|DEPARTURE_DELAY|TAXI_OUT|WHEELS_OFF|SCHEDULED_TIME|ELAPSED_TIME|AIR_TIME|DISTANCE|WHEELS_ON|TAXI_IN|SCHEDULED_ARRIVAL|ARRIVAL_TIME|ARRIVAL_DELAY|DIVERTED|CANCELLED|CANCELLATION_REASON|AIR_SYSTEM_DELAY|SECURITY_DELAY|AIRLINE_DELAY|LATE_AIRCRAFT_DELAY|WEATHER_DELAY|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|2015|    1|  1|          4|     AS|           98|     N407AS|           ANC|                SEA|                  5|          2354|            -11|      21|        15|           205|         194|     169|    1448|      404|      4|              430|         408|          -22|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AA|         2336|     N3KUAA|           LAX|                PBI|                 10|             2|             -8|      12|        14|           280|         279|     263|    2330|      737|      4|              750|         741|           -9|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     US|          840|     N171US|           SFO|                CLT|                 20|            18|             -2|      16|        34|           286|         293|     266|    2296|      800|     11|              806|         811|            5|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AA|          258|     N3HYAA|           LAX|                MIA|                 20|            15|             -5|      15|        30|           285|         281|     258|    2342|      748|      8|              805|         756|           -9|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AS|          135|     N527AS|           SEA|                ANC|                 25|            24|             -1|      11|        35|           235|         215|     199|    1448|      254|      5|              320|         259|          -21|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     DL|          806|     N3730B|           SFO|                MSP|                 25|            20|             -5|      18|        38|           217|         230|     206|    1589|      604|      6|              602|         610|            8|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     NK|          612|     N635NK|           LAS|                MSP|                 25|            19|             -6|      11|        30|           181|         170|     154|    1299|      504|      5|              526|         509|          -17|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     US|         2013|     N584UW|           LAX|                CLT|                 30|            44|             14|      13|        57|           273|         249|     228|    2125|      745|      8|              803|         753|          -10|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AA|         1112|     N3LAAA|           SFO|                DFW|                 30|            19|            -11|      17|        36|           195|         193|     173|    1464|      529|      3|              545|         532|          -13|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     DL|         1173|     N826DN|           LAS|                ATL|                 30|            33|              3|      12|        45|           221|         203|     186|    1747|      651|      5|              711|         656|          -15|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     DL|         2336|     N958DN|           DEN|                ATL|                 30|            24|             -6|      12|        36|           173|         149|     133|    1199|      449|      4|              523|         453|          -30|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AA|         1674|     N853AA|           LAS|                MIA|                 35|            27|             -8|      21|        48|           268|         266|     238|    2174|      746|      7|              803|         753|          -10|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     DL|         1434|     N547US|           LAX|                MSP|                 35|            35|              0|      18|        53|           214|         210|     188|    1535|      601|      4|              609|         605|           -4|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     DL|         2324|     N3751B|           SLC|                ATL|                 40|            34|             -6|      18|        52|           215|         199|     176|    1590|      548|      5|              615|         553|          -22|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     DL|         2440|     N651DL|           SEA|                MSP|                 40|            39|             -1|      28|       107|           189|         198|     166|    1399|      553|      4|              549|         557|            8|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AS|          108|     N309AS|           ANC|                SEA|                 45|            41|             -4|      17|        58|           204|         194|     173|    1448|      451|      4|              509|         455|          -14|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     DL|         1560|     N3743H|           ANC|                SEA|                 45|            31|            -14|      25|        56|           210|         200|     171|    1448|      447|      4|              515|         451|          -24|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     UA|         1197|     N78448|           SFO|                IAH|                 48|            42|             -6|      11|        53|           218|         217|     199|    1635|      612|      7|              626|         619|           -7|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     AS|          122|     N413AS|           ANC|                PDX|                 50|            46|             -4|      11|        57|           215|         201|     187|    1542|      504|      3|              525|         507|          -18|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "|2015|    1|  1|          4|     DL|         1670|     N806DN|           PDX|                MSP|                 50|            45|             -5|       9|        54|           193|         186|     171|    1426|      545|      6|              603|         551|          -12|       0|        0|               null|            null|          null|         null|               null|         null|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_df = (data.filter(col('MONTH').isin(6, 7, 8))\n",
    "            .withColumn('dis_tiempo_aire', col('DISTANCE') / col('AIR_TIME'))\n",
    ").select(\n",
    "    col('AIRLINE'),\n",
    "    col('dis_tiempo_aire')\n",
    ").where(col('AIRLINE').isin('AA', 'DL', 'AS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualizamos el plan lógico y el plan físico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter 'AIRLINE IN (AA,DL,AS)\n",
      "+- Project [AIRLINE#784, dis_tiempo_aire#968]\n",
      "   +- Project [YEAR#780, MONTH#781, DAY#782, DAY_OF_WEEK#783, AIRLINE#784, FLIGHT_NUMBER#785, TAIL_NUMBER#786, ORIGIN_AIRPORT#787, DESTINATION_AIRPORT#788, SCHEDULED_DEPARTURE#789, DEPARTURE_TIME#790, DEPARTURE_DELAY#791, TAXI_OUT#792, WHEELS_OFF#793, SCHEDULED_TIME#794, ELAPSED_TIME#795, AIR_TIME#796, DISTANCE#797, WHEELS_ON#798, TAXI_IN#799, SCHEDULED_ARRIVAL#800, ARRIVAL_TIME#801, ARRIVAL_DELAY#802, DIVERTED#803, ... 8 more fields]\n",
      "      +- Filter MONTH#781 IN (6,7,8)\n",
      "         +- Relation [YEAR#780,MONTH#781,DAY#782,DAY_OF_WEEK#783,AIRLINE#784,FLIGHT_NUMBER#785,TAIL_NUMBER#786,ORIGIN_AIRPORT#787,DESTINATION_AIRPORT#788,SCHEDULED_DEPARTURE#789,DEPARTURE_TIME#790,DEPARTURE_DELAY#791,TAXI_OUT#792,WHEELS_OFF#793,SCHEDULED_TIME#794,ELAPSED_TIME#795,AIR_TIME#796,DISTANCE#797,WHEELS_ON#798,TAXI_IN#799,SCHEDULED_ARRIVAL#800,ARRIVAL_TIME#801,ARRIVAL_DELAY#802,DIVERTED#803,... 7 more fields] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "AIRLINE: string, dis_tiempo_aire: double\n",
      "Filter AIRLINE#784 IN (AA,DL,AS)\n",
      "+- Project [AIRLINE#784, dis_tiempo_aire#968]\n",
      "   +- Project [YEAR#780, MONTH#781, DAY#782, DAY_OF_WEEK#783, AIRLINE#784, FLIGHT_NUMBER#785, TAIL_NUMBER#786, ORIGIN_AIRPORT#787, DESTINATION_AIRPORT#788, SCHEDULED_DEPARTURE#789, DEPARTURE_TIME#790, DEPARTURE_DELAY#791, TAXI_OUT#792, WHEELS_OFF#793, SCHEDULED_TIME#794, ELAPSED_TIME#795, AIR_TIME#796, DISTANCE#797, WHEELS_ON#798, TAXI_IN#799, SCHEDULED_ARRIVAL#800, ARRIVAL_TIME#801, ARRIVAL_DELAY#802, DIVERTED#803, ... 8 more fields]\n",
      "      +- Filter MONTH#781 IN (6,7,8)\n",
      "         +- Relation [YEAR#780,MONTH#781,DAY#782,DAY_OF_WEEK#783,AIRLINE#784,FLIGHT_NUMBER#785,TAIL_NUMBER#786,ORIGIN_AIRPORT#787,DESTINATION_AIRPORT#788,SCHEDULED_DEPARTURE#789,DEPARTURE_TIME#790,DEPARTURE_DELAY#791,TAXI_OUT#792,WHEELS_OFF#793,SCHEDULED_TIME#794,ELAPSED_TIME#795,AIR_TIME#796,DISTANCE#797,WHEELS_ON#798,TAXI_IN#799,SCHEDULED_ARRIVAL#800,ARRIVAL_TIME#801,ARRIVAL_DELAY#802,DIVERTED#803,... 7 more fields] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [AIRLINE#784, (cast(DISTANCE#797 as double) / cast(AIR_TIME#796 as double)) AS dis_tiempo_aire#968]\n",
      "+- Filter (MONTH#781 IN (6,7,8) AND AIRLINE#784 IN (AA,DL,AS))\n",
      "   +- Relation [YEAR#780,MONTH#781,DAY#782,DAY_OF_WEEK#783,AIRLINE#784,FLIGHT_NUMBER#785,TAIL_NUMBER#786,ORIGIN_AIRPORT#787,DESTINATION_AIRPORT#788,SCHEDULED_DEPARTURE#789,DEPARTURE_TIME#790,DEPARTURE_DELAY#791,TAXI_OUT#792,WHEELS_OFF#793,SCHEDULED_TIME#794,ELAPSED_TIME#795,AIR_TIME#796,DISTANCE#797,WHEELS_ON#798,TAXI_IN#799,SCHEDULED_ARRIVAL#800,ARRIVAL_TIME#801,ARRIVAL_DELAY#802,DIVERTED#803,... 7 more fields] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [AIRLINE#784, (cast(DISTANCE#797 as double) / cast(AIR_TIME#796 as double)) AS dis_tiempo_aire#968]\n",
      "+- *(1) Filter (MONTH#781 IN (6,7,8) AND AIRLINE#784 IN (AA,DL,AS))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [MONTH#781,AIRLINE#784,AIR_TIME#796,DISTANCE#797] Batched: true, DataFilters: [MONTH#781 IN (6,7,8), AIRLINE#784 IN (AA,DL,AS)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/usr/Documents/GitHub/lab-pyspark/data/data/vuelos.parquet], PartitionFilters: [], PushedFilters: [In(MONTH, [6,7,8]), In(AIRLINE, [AA,AS,DL])], ReadSchema: struct<MONTH:int,AIRLINE:string,AIR_TIME:int,DISTANCE:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nuevo_df.explain(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
