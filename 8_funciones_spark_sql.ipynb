{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones de fecha y hora**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['JAVA_HOME'] = \"C:/Program Files/Java/jdk-11\"\n",
    "os.environ['PYSPARK_PYTHON'] = \"C:/Users/usr/anaconda3/envs/pyspark_env/python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"C:/Users/usr/anaconda3/envs/pyspark_env/python.exe\"\n",
    "os.environ['HADOOP_HOME'] = \"C:/hadoop-3.4.0\"\n",
    "os.environ['HADOOP_COMMON_LIB_NATIVE_DIR'] = \"C:/hadoop-3.4.0/lib/native\"\n",
    "os.environ['PATH'] += os.pathsep + \"C:/hadoop-3.4.0/bin\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part1 = spark.read.parquet('./data/convertir/part-00000-6b84f509-ffb8-43d4-820e-866fb80c0d08-c000.snappy.parquet')\n",
    "data_part2 = spark.read.parquet('./data/convertir/part-00001-6b84f509-ffb8-43d4-820e-866fb80c0d08-c000.snappy.parquet')\n",
    "\n",
    "data = data_part1.union(data_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- date_str: string (nullable = true)\n",
      " |-- ts_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------------+\n",
      "|date      |timestamp              |date_str  |ts_str          |\n",
      "+----------+-----------------------+----------+----------------+\n",
      "|2021-01-01|2021-01-01 20:10:50.723|01-01-2021|18-08-2021 46:58|\n",
      "+----------+-----------------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos a convertir de formato string a formato date y timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.select(\n",
    "    to_date(col('date')).alias('date1'),\n",
    "    to_timestamp(col('timestamp')).alias('ts1'),\n",
    "    to_date(col('date_str'), 'dd-MM-yyyy').alias('date2'),\n",
    "    to_timestamp(col('ts_str'), 'dd-MM-yyyy mm:ss').alias('ts2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+-------------------+\n",
      "|date1     |ts1                    |date2     |ts2                |\n",
      "+----------+-----------------------+----------+-------------------+\n",
      "|2021-01-01|2021-01-01 20:10:50.723|2021-01-01|2021-08-18 00:46:58|\n",
      "+----------+-----------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date1: date (nullable = true)\n",
      " |-- ts1: timestamp (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- ts2: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ahora vamos a darle formato a una fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|date_format(date1, dd-MM-yyyy)|\n",
      "+------------------------------+\n",
      "|                    01-01-2021|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.select(\n",
    "    date_format(col('date1'), 'dd-MM-yyyy')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cálculos con fechas y horas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('./data/calculo/calculo.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+-------------------+\n",
      "|nombre|fecha_ingreso|fecha_salida|       baja_sistema|\n",
      "+------+-------------+------------+-------------------+\n",
      "|  Jose|   2021-01-01|  2021-11-14|2021-10-14 15:35:59|\n",
      "|Mayara|   2021-02-06|  2021-11-25|2021-11-25 10:35:55|\n",
      "+------+-------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, last_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+--------------+\n",
      "|nombre|dias|      meses|ultimo_dia_mes|\n",
      "+------+----+-----------+--------------+\n",
      "|  Jose| 317|10.41935484|    2021-11-30|\n",
      "|Mayara| 292| 9.61290323|    2021-11-30|\n",
      "+------+----+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('nombre'),\n",
    "    datediff(col('fecha_salida'), col('fecha_ingreso')).alias('dias'),\n",
    "    months_between(col('fecha_salida'), col('fecha_ingreso')).alias('meses'),\n",
    "    last_day(col('fecha_salida')).alias('ultimo_dia_mes')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sumar y restar fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------+-----------+\n",
      "|nombre|fecha_ingreso|mas_14_dias|menos_1_dia|\n",
      "+------+-------------+-----------+-----------+\n",
      "|  Jose|   2021-01-01| 2021-01-15| 2020-12-31|\n",
      "|Mayara|   2021-02-06| 2021-02-20| 2021-02-05|\n",
      "+------+-------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('nombre'),\n",
    "    col('fecha_ingreso'),\n",
    "    date_add(col('fecha_ingreso'), 14).alias('mas_14_dias'),\n",
    "    date_sub(col('fecha_ingreso'), 1).alias('menos_1_dia')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extraer valores específicos de una columna date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, dayofyear, hour, minute, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|       baja_sistema|year(baja_sistema)|month(baja_sistema)|dayofmonth(baja_sistema)|dayofyear(baja_sistema)|hour(baja_sistema)|minute(baja_sistema)|second(baja_sistema)|\n",
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|2021-10-14 15:35:59|              2021|                 10|                      14|                    287|                15|                  35|                  59|\n",
      "|2021-11-25 10:35:55|              2021|                 11|                      25|                    329|                10|                  35|                  55|\n",
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('baja_sistema'),\n",
    "    year(col('baja_sistema')),\n",
    "    month(col('baja_sistema')),\n",
    "    dayofmonth(col('baja_sistema')),\n",
    "    dayofyear(col('baja_sistema')),\n",
    "    hour(col('baja_sistema')),\n",
    "    minute(col('baja_sistema')),\n",
    "    second(col('baja_sistema'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones para trabajo con strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('./data/data/data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformaciones string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| nombre|\n",
      "+-------+\n",
      "| Spark |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tiene espacios en blanco al principio y al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ltrim, rtrim, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "| ltrim| rtrim| trim|\n",
      "+------+------+-----+\n",
      "|Spark | Spark|Spark|\n",
      "+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    ltrim(col('nombre')).alias('ltrim'), # Elimina los espacios a la izquierda.\n",
    "    rtrim(col('nombre')).alias('rtrim'), # Elimina los espacios a la derecha.\n",
    "    trim(col('nombre')).alias('trim') # Elimina los espacios a la izquierda y derecha.\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rellenar string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lpad, rpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    lpad|    rpad|\n",
      "+--------+--------+\n",
      "|---Spark|Spark===|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    trim(col('nombre')).alias('trim'),\n",
    ").select(\n",
    "    lpad(col('trim'), 8, '-').alias('lpad'), # Agrega guiones a la izquierda.\n",
    "    rpad(col('trim'), 8, '=').alias('rpad') # Agrega igual al final.\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([('Spark', 'es', 'maravilloso')], ['sujeto', 'verbo', 'adjetivo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+\n",
      "|sujeto|verbo|   adjetivo|\n",
      "+------+-----+-----------+\n",
      "| Spark|   es|maravilloso|\n",
      "+------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concatenación, mayúsculas, minúsculas y reverso de string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, lower, upper, initcap, reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               frase|           minuscula|           mayuscula|             initcap|             reversa|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Spark es maravilloso|spark es maravilloso|SPARK ES MARAVILLOSO|Spark Es Maravilloso|osollivaram se krapS|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\n",
    "    concat_ws(' ', col('sujeto'), col('verbo'), col('adjetivo')).alias('frase')\n",
    ").select(\n",
    "    col('frase'),\n",
    "    lower(col('frase')).alias('minuscula'),\n",
    "    upper(col('frase')).alias('mayuscula'),\n",
    "    initcap(col('frase')).alias('initcap'),\n",
    "    reverse(col('frase')).alias('reversa')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame([(' voy a casa por mis llaves',)], ['frase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|frase                     |\n",
      "+--------------------------+\n",
      "| voy a casa por mis llaves|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|nueva_frase             |\n",
      "+------------------------+\n",
      "| ir a casa ir mis llaves|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\n",
    "    regexp_replace(col('frase'), 'voy|por', 'ir').alias('nueva_frase') # Reemplaza \"voy\" o \"por\" por ir.\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones para el trabajo con colecciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('./data/data_collections/parquet/part-00000-96f39196-ef97-4a14-926e-b24a86c2e32d-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------+\n",
      "|dia  |tareas                                      |\n",
      "+-----+--------------------------------------------+\n",
      "|lunes|[hacer la tarea, buscar agua, lavar el auto]|\n",
      "+-----+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dia: string (nullable = true)\n",
      " |-- tareas: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size, sort_array, array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------+-----------+\n",
      "|tamaño|array_ordenado                              |buscar_agua|\n",
      "+------+--------------------------------------------+-----------+\n",
      "|3     |[buscar agua, hacer la tarea, lavar el auto]|true       |\n",
      "+------+--------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    size(col('tareas')).alias('tamaño'),\n",
    "    sort_array(col('tareas')).alias('array_ordenado'),\n",
    "    array_contains(col('tareas'), 'buscar agua').alias('buscar_agua')\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|dia  |tareas        |\n",
      "+-----+--------------+\n",
      "|lunes|hacer la tarea|\n",
      "|lunes|buscar agua   |\n",
      "|lunes|lavar el auto |\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    col('dia'),\n",
    "    explode(col('tareas')).alias('tareas')\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Formato JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_str = spark.read.parquet('./data/data_collections/JSON/part-00000-9f0e2b16-0b50-4212-9b83-654e1b8dd137-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|tareas_str                                                                 |\n",
      "+---------------------------------------------------------------------------+\n",
      "|{\"dia\": \"lunes\",\"tareas\": [\"hacer la tarea\",\"buscar agua\",\"lavar el auto\"]}|\n",
      "+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df_str.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tareas_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df_str.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convertimos en estructura Spark, para ello debemos describir su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_json = StructType(\n",
    "    [\n",
    "        StructField('dia', StringType(), True),\n",
    "        StructField('tareas', ArrayType(StringType()), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = json_df_str.select(\n",
    "    from_json(col('tareas_str'), schema_json).alias('por_hacer')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- por_hacer: struct (nullable = true)\n",
      " |    |-- dia: string (nullable = true)\n",
      " |    |-- tareas: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------------------------------+--------------+\n",
      "|por_hacer.dia|por_hacer.tareas                            |primer_tarea  |\n",
      "+-------------+--------------------------------------------+--------------+\n",
      "|lunes        |[hacer la tarea, buscar agua, lavar el auto]|hacer la tarea|\n",
      "+-------------+--------------------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(\n",
    "    col('por_hacer').getItem('dia'),\n",
    "    col('por_hacer').getItem('tareas'),\n",
    "    col('por_hacer').getItem('tareas').getItem(0).alias('primer_tarea')\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|to_json(por_hacer)                                                       |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"dia\":\"lunes\",\"tareas\":[\"hacer la tarea\",\"buscar agua\",\"lavar el auto\"]}|\n",
      "+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(\n",
    "    to_json(col('por_hacer'))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones when, coalesce y lit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('./data/data/when_coalesce_lit.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|nombre|pago|\n",
      "+------+----+\n",
      "|  Jose|   1|\n",
      "| Julia|   2|\n",
      "| Katia|   1|\n",
      "|  null|   3|\n",
      "|  Raul|   3|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|nombre|       pago|\n",
      "+------+-----------+\n",
      "|  Jose|     pagado|\n",
      "| Julia|  sin pagar|\n",
      "| Katia|     pagado|\n",
      "|  null|sin iniciar|\n",
      "|  Raul|sin iniciar|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    col('nombre'),\n",
    "    when(col('pago') == 1, 'pagado').when(col('pago') == 2, 'sin pagar').otherwise('sin iniciar').alias('pago')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con coalesce toma uno o más valores de columna y devuelve el primero que no es nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|    nombre|\n",
      "+----------+\n",
      "|      Jose|\n",
      "|     Julia|\n",
      "|     Katia|\n",
      "|sin nombre|\n",
      "|      Raul|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    coalesce(col('nombre'), lit('sin nombre')).alias('nombre') \n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones definidas por el usuario (UDF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creamos una función para calcular el cubo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_cubo(n):\n",
    "    return n * n * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Registramos nuestra función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indicamos el nombre que le queremos dar, el nombre de la función que escribimos previamente y el tipo de dato que va a retornar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.f_cubo(n)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register('cubo', f_cubo, LongType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testeamos la función."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creamos una lista a través de una vista temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1,10).createOrReplaceTempView('df_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|cubo|\n",
      "+---+----+\n",
      "|  1|   1|\n",
      "|  2|   8|\n",
      "|  3|  27|\n",
      "|  4|  64|\n",
      "|  5| 125|\n",
      "|  6| 216|\n",
      "|  7| 343|\n",
      "|  8| 512|\n",
      "|  9| 729|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT id, cubo(id) AS cubo FROM df_temp').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otra alternativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bienvenida(nombre):\n",
    "    return ('Hola {}'.format(nombre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos a dar la bienvenida a cada parámetro x que tome la función lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bienvenida_udf = udf(lambda x: bienvenida(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nombre = spark.createDataFrame([('Jose',), ('Julia',)], ['nombre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|nombre|\n",
      "+------+\n",
      "|  Jose|\n",
      "| Julia|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nombre.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|nombre|bien_nombre|\n",
      "+------+-----------+\n",
      "|  Jose|  Hola Jose|\n",
      "| Julia| Hola Julia|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nombre.select(\n",
    "    col('nombre'),\n",
    "    bienvenida_udf(col('nombre')).alias('bien_nombre')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3ª alternativa para crear un udf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def mayuscula(s):\n",
    "    return s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|nombre|may_nombre|\n",
      "+------+----------+\n",
      "|  Jose|      JOSE|\n",
      "| Julia|     JULIA|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nombre.select(\n",
    "    col('nombre'),\n",
    "    mayuscula(col('nombre')).alias('may_nombre')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas udf (udf vectorizadas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tenemos que especificar que el parámetro que recibe la función es de tipo pandas Series y el tipo de parámetro que va a devolver (->)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubo_pandas(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubo_udf = pandas_udf(cubo_pandas, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     8\n",
      "2    27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(cubo_pandas(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|cubo_pandas|\n",
      "+---+-----------+\n",
      "|  0|          0|\n",
      "|  1|          1|\n",
      "|  2|          8|\n",
      "|  3|         27|\n",
      "|  4|         64|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('id'), # La columna 'id' nos la crear por defecto.\n",
    "    cubo_udf(col('id')).alias('cubo_pandas')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones de ventana**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('./data/funciones_ventana.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+\n",
      "| nombre|edad|departamento|evaluacion|\n",
      "+-------+----+------------+----------+\n",
      "| Lazaro|  45|      letras|        98|\n",
      "|   Raul|  24|  matemática|        76|\n",
      "|  Maria|  34|  matemática|        27|\n",
      "|   Jose|  30|     química|        78|\n",
      "| Susana|  51|     química|        98|\n",
      "|   Juan|  44|      letras|        89|\n",
      "|  Julia|  55|      letras|        92|\n",
      "|  Kadir|  38|arquitectura|        39|\n",
      "| Lilian|  23|arquitectura|        94|\n",
      "|   Rosa|  26|      letras|        91|\n",
      "|   Aian|  50|  matemática|        73|\n",
      "|Yaneisy|  29|      letras|        89|\n",
      "|Enrique|  40|     química|        92|\n",
      "|    Jon|  25|arquitectura|        78|\n",
      "|  Luisa|  39|arquitectura|        94|\n",
      "+-------+----+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trabajadores con evaluación más alta por departamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, row_number, rank, dense_rank, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Especificación de ventana: vamos a neceitar particionar los datos y ordenarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creamos grupos de trabajadores por departamentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy('departamento').orderBy(desc('evaluacion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- row_number: número de filas secuencial comenzando desde 1 hasta el resultado de cada partición de ventana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----------+\n",
      "| nombre|edad|departamento|evaluacion|row_number|\n",
      "+-------+----+------------+----------+----------+\n",
      "| Lilian|  23|arquitectura|        94|         1|\n",
      "|  Luisa|  39|arquitectura|        94|         2|\n",
      "| Lazaro|  45|      letras|        98|         1|\n",
      "|  Julia|  55|      letras|        92|         2|\n",
      "|   Raul|  24|  matemática|        76|         1|\n",
      "|   Aian|  50|  matemática|        73|         2|\n",
      "| Susana|  51|     química|        98|         1|\n",
      "|Enrique|  40|     química|        92|         2|\n",
      "+-------+----+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('row_number', row_number().over(windowSpec)).filter(col('row_number').isin(1, 2)).show() # Agrega una columna al DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rank: rango al resultado dentro de una partición de ventana, esta función deja huecos en el rango cuando hay empates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----+\n",
      "| nombre|edad|departamento|evaluacion|rank|\n",
      "+-------+----+------------+----------+----+\n",
      "| Lilian|  23|arquitectura|        94|   1|\n",
      "|  Luisa|  39|arquitectura|        94|   1|\n",
      "|    Jon|  25|arquitectura|        78|   3|\n",
      "|  Kadir|  38|arquitectura|        39|   4|\n",
      "| Lazaro|  45|      letras|        98|   1|\n",
      "|  Julia|  55|      letras|        92|   2|\n",
      "|   Rosa|  26|      letras|        91|   3|\n",
      "|   Juan|  44|      letras|        89|   4|\n",
      "|Yaneisy|  29|      letras|        89|   4|\n",
      "|   Raul|  24|  matemática|        76|   1|\n",
      "|   Aian|  50|  matemática|        73|   2|\n",
      "|  Maria|  34|  matemática|        27|   3|\n",
      "| Susana|  51|     química|        98|   1|\n",
      "|Enrique|  40|     química|        92|   2|\n",
      "|   Jose|  30|     química|        78|   3|\n",
      "+-------+----+------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('rank', rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si no queremos dejar huecos trabajamos con la función dense_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+----------+\n",
      "| nombre|edad|departamento|evaluacion|dense_rank|\n",
      "+-------+----+------------+----------+----------+\n",
      "| Lilian|  23|arquitectura|        94|         1|\n",
      "|  Luisa|  39|arquitectura|        94|         1|\n",
      "|    Jon|  25|arquitectura|        78|         2|\n",
      "|  Kadir|  38|arquitectura|        39|         3|\n",
      "| Lazaro|  45|      letras|        98|         1|\n",
      "|  Julia|  55|      letras|        92|         2|\n",
      "|   Rosa|  26|      letras|        91|         3|\n",
      "|   Juan|  44|      letras|        89|         4|\n",
      "|Yaneisy|  29|      letras|        89|         4|\n",
      "|   Raul|  24|  matemática|        76|         1|\n",
      "|   Aian|  50|  matemática|        73|         2|\n",
      "|  Maria|  34|  matemática|        27|         3|\n",
      "| Susana|  51|     química|        98|         1|\n",
      "|Enrique|  40|     química|        92|         2|\n",
      "|   Jose|  30|     química|        78|         3|\n",
      "+-------+----+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('dense_rank', dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agregaciones con funciones de ventana, no es necesario utilizar la clausula orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpecAgg = Window.partitionBy('departamento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+----------+---+---+------------------+----------+\n",
      "| nombre|edad|departamento|evaluacion|min|max|               avg|row_number|\n",
      "+-------+----+------------+----------+---+---+------------------+----------+\n",
      "| Lilian|  23|arquitectura|        94| 39| 94|             76.25|         1|\n",
      "|  Luisa|  39|arquitectura|        94| 39| 94|             76.25|         2|\n",
      "|    Jon|  25|arquitectura|        78| 39| 94|             76.25|         3|\n",
      "|  Kadir|  38|arquitectura|        39| 39| 94|             76.25|         4|\n",
      "| Lazaro|  45|      letras|        98| 89| 98|              91.8|         1|\n",
      "|  Julia|  55|      letras|        92| 89| 98|              91.8|         2|\n",
      "|   Rosa|  26|      letras|        91| 89| 98|              91.8|         3|\n",
      "|   Juan|  44|      letras|        89| 89| 98|              91.8|         4|\n",
      "|Yaneisy|  29|      letras|        89| 89| 98|              91.8|         5|\n",
      "|   Raul|  24|  matemática|        76| 27| 76|58.666666666666664|         1|\n",
      "|   Aian|  50|  matemática|        73| 27| 76|58.666666666666664|         2|\n",
      "|  Maria|  34|  matemática|        27| 27| 76|58.666666666666664|         3|\n",
      "| Susana|  51|     química|        98| 78| 98| 89.33333333333333|         1|\n",
      "|Enrique|  40|     química|        92| 78| 98| 89.33333333333333|         2|\n",
      "|   Jose|  30|     química|        78| 78| 98| 89.33333333333333|         3|\n",
      "+-------+----+------------+----------+---+---+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.withColumn('min', min('evaluacion').over(windowSpecAgg))\n",
    ".withColumn('max', max('evaluacion').over(windowSpecAgg))\n",
    ".withColumn('avg', avg('evaluacion').over(windowSpecAgg))\n",
    ".withColumn('row_number', row_number().over(windowSpec)) # row_number() si necesita una especificación de ventana con clausula orderBy.\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
