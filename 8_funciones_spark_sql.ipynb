{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones de fecha y hora**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['JAVA_HOME'] = \"C:/Program Files/Java/jdk-11\"\n",
    "os.environ['PYSPARK_PYTHON'] = \"C:/Users/usr/anaconda3/envs/pyspark_env/python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"C:/Users/usr/anaconda3/envs/pyspark_env/python.exe\"\n",
    "os.environ['HADOOP_HOME'] = \"C:/hadoop-3.4.0\"\n",
    "os.environ['HADOOP_COMMON_LIB_NATIVE_DIR'] = \"C:/hadoop-3.4.0/lib/native\"\n",
    "os.environ['PATH'] += os.pathsep + \"C:/hadoop-3.4.0/bin\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part1 = spark.read.parquet('./data/convertir/part-00000-6b84f509-ffb8-43d4-820e-866fb80c0d08-c000.snappy.parquet')\n",
    "data_part2 = spark.read.parquet('./data/convertir/part-00001-6b84f509-ffb8-43d4-820e-866fb80c0d08-c000.snappy.parquet')\n",
    "\n",
    "data = data_part1.union(data_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- date_str: string (nullable = true)\n",
      " |-- ts_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------------+\n",
      "|date      |timestamp              |date_str  |ts_str          |\n",
      "+----------+-----------------------+----------+----------------+\n",
      "|2021-01-01|2021-01-01 20:10:50.723|01-01-2021|18-08-2021 46:58|\n",
      "+----------+-----------------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos a convertir de formato string a formato date y timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.select(\n",
    "    to_date(col('date')).alias('date1'),\n",
    "    to_timestamp(col('timestamp')).alias('ts1'),\n",
    "    to_date(col('date_str'), 'dd-MM-yyyy').alias('date2'),\n",
    "    to_timestamp(col('ts_str'), 'dd-MM-yyyy mm:ss').alias('ts2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+-------------------+\n",
      "|date1     |ts1                    |date2     |ts2                |\n",
      "+----------+-----------------------+----------+-------------------+\n",
      "|2021-01-01|2021-01-01 20:10:50.723|2021-01-01|2021-08-18 00:46:58|\n",
      "+----------+-----------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date1: date (nullable = true)\n",
      " |-- ts1: timestamp (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- ts2: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ahora vamos a darle formato a una fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|date_format(date1, dd-MM-yyyy)|\n",
      "+------------------------------+\n",
      "|                    01-01-2021|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.select(\n",
    "    date_format(col('date1'), 'dd-MM-yyyy')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cálculos con fechas y horas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('./data/calculo/calculo.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+-------------------+\n",
      "|nombre|fecha_ingreso|fecha_salida|       baja_sistema|\n",
      "+------+-------------+------------+-------------------+\n",
      "|  Jose|   2021-01-01|  2021-11-14|2021-10-14 15:35:59|\n",
      "|Mayara|   2021-02-06|  2021-11-25|2021-11-25 10:35:55|\n",
      "+------+-------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, last_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+--------------+\n",
      "|nombre|dias|      meses|ultimo_dia_mes|\n",
      "+------+----+-----------+--------------+\n",
      "|  Jose| 317|10.41935484|    2021-11-30|\n",
      "|Mayara| 292| 9.61290323|    2021-11-30|\n",
      "+------+----+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('nombre'),\n",
    "    datediff(col('fecha_salida'), col('fecha_ingreso')).alias('dias'),\n",
    "    months_between(col('fecha_salida'), col('fecha_ingreso')).alias('meses'),\n",
    "    last_day(col('fecha_salida')).alias('ultimo_dia_mes')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sumar y restar fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------+-----------+\n",
      "|nombre|fecha_ingreso|mas_14_dias|menos_1_dia|\n",
      "+------+-------------+-----------+-----------+\n",
      "|  Jose|   2021-01-01| 2021-01-15| 2020-12-31|\n",
      "|Mayara|   2021-02-06| 2021-02-20| 2021-02-05|\n",
      "+------+-------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('nombre'),\n",
    "    col('fecha_ingreso'),\n",
    "    date_add(col('fecha_ingreso'), 14).alias('mas_14_dias'),\n",
    "    date_sub(col('fecha_ingreso'), 1).alias('menos_1_dia')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extraer valores específicos de una columna date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, dayofyear, hour, minute, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|       baja_sistema|year(baja_sistema)|month(baja_sistema)|dayofmonth(baja_sistema)|dayofyear(baja_sistema)|hour(baja_sistema)|minute(baja_sistema)|second(baja_sistema)|\n",
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|2021-10-14 15:35:59|              2021|                 10|                      14|                    287|                15|                  35|                  59|\n",
      "|2021-11-25 10:35:55|              2021|                 11|                      25|                    329|                10|                  35|                  55|\n",
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('baja_sistema'),\n",
    "    year(col('baja_sistema')),\n",
    "    month(col('baja_sistema')),\n",
    "    dayofmonth(col('baja_sistema')),\n",
    "    dayofyear(col('baja_sistema')),\n",
    "    hour(col('baja_sistema')),\n",
    "    minute(col('baja_sistema')),\n",
    "    second(col('baja_sistema'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones para trabajo con strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('./data/data/data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformaciones string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| nombre|\n",
      "+-------+\n",
      "| Spark |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tiene espacios en blanco al principio y al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ltrim, rtrim, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "| ltrim| rtrim| trim|\n",
      "+------+------+-----+\n",
      "|Spark | Spark|Spark|\n",
      "+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    ltrim(col('nombre')).alias('ltrim'), # Elimina los espacios a la izquierda.\n",
    "    rtrim(col('nombre')).alias('rtrim'), # Elimina los espacios a la derecha.\n",
    "    trim(col('nombre')).alias('trim') # Elimina los espacios a la izquierda y derecha.\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rellenar string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lpad, rpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    lpad|    rpad|\n",
      "+--------+--------+\n",
      "|---Spark|Spark===|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    trim(col('nombre')).alias('trim'),\n",
    ").select(\n",
    "    lpad(col('trim'), 8, '-').alias('lpad'), # Agrega guiones a la izquierda.\n",
    "    rpad(col('trim'), 8, '=').alias('rpad') # Agrega igual al final.\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([('Spark', 'es', 'maravilloso')], ['sujeto', 'verbo', 'adjetivo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+\n",
      "|sujeto|verbo|   adjetivo|\n",
      "+------+-----+-----------+\n",
      "| Spark|   es|maravilloso|\n",
      "+------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concatenación, mayúsculas, minúsculas y reverso de string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, lower, upper, initcap, reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               frase|           minuscula|           mayuscula|             initcap|             reversa|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Spark es maravilloso|spark es maravilloso|SPARK ES MARAVILLOSO|Spark Es Maravilloso|osollivaram se krapS|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\n",
    "    concat_ws(' ', col('sujeto'), col('verbo'), col('adjetivo')).alias('frase')\n",
    ").select(\n",
    "    col('frase'),\n",
    "    lower(col('frase')).alias('minuscula'),\n",
    "    upper(col('frase')).alias('mayuscula'),\n",
    "    initcap(col('frase')).alias('initcap'),\n",
    "    reverse(col('frase')).alias('reversa')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame([(' voy a casa por mis llaves',)], ['frase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|frase                     |\n",
      "+--------------------------+\n",
      "| voy a casa por mis llaves|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|nueva_frase             |\n",
      "+------------------------+\n",
      "| ir a casa ir mis llaves|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\n",
    "    regexp_replace(col('frase'), 'voy|por', 'ir').alias('nueva_frase') # Reemplaza \"voy\" o \"por\" por ir.\n",
    ").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
