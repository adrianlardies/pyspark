{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones de fecha y hora**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['JAVA_HOME'] = \"C:/Program Files/Java/jdk-11\"\n",
    "os.environ['PYSPARK_PYTHON'] = \"C:/Users/usr/anaconda3/envs/pyspark_env/python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"C:/Users/usr/anaconda3/envs/pyspark_env/python.exe\"\n",
    "os.environ['HADOOP_HOME'] = \"C:/hadoop-3.4.0\"\n",
    "os.environ['HADOOP_COMMON_LIB_NATIVE_DIR'] = \"C:/hadoop-3.4.0/lib/native\"\n",
    "os.environ['PATH'] += os.pathsep + \"C:/hadoop-3.4.0/bin\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_part1 = spark.read.parquet('./data/convertir/part-00000-6b84f509-ffb8-43d4-820e-866fb80c0d08-c000.snappy.parquet')\n",
    "data_part2 = spark.read.parquet('./data/convertir/part-00001-6b84f509-ffb8-43d4-820e-866fb80c0d08-c000.snappy.parquet')\n",
    "\n",
    "data = data_part1.union(data_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- date_str: string (nullable = true)\n",
      " |-- ts_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------------+\n",
      "|date      |timestamp              |date_str  |ts_str          |\n",
      "+----------+-----------------------+----------+----------------+\n",
      "|2021-01-01|2021-01-01 20:10:50.723|01-01-2021|18-08-2021 46:58|\n",
      "+----------+-----------------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos a convertir de formato string a formato date y timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.select(\n",
    "    to_date(col('date')).alias('date1'),\n",
    "    to_timestamp(col('timestamp')).alias('ts1'),\n",
    "    to_date(col('date_str'), 'dd-MM-yyyy').alias('date2'),\n",
    "    to_timestamp(col('ts_str'), 'dd-MM-yyyy mm:ss').alias('ts2')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+-------------------+\n",
      "|date1     |ts1                    |date2     |ts2                |\n",
      "+----------+-----------------------+----------+-------------------+\n",
      "|2021-01-01|2021-01-01 20:10:50.723|2021-01-01|2021-08-18 00:46:58|\n",
      "+----------+-----------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date1: date (nullable = true)\n",
      " |-- ts1: timestamp (nullable = true)\n",
      " |-- date2: date (nullable = true)\n",
      " |-- ts2: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ahora vamos a darle formato a una fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|date_format(date1, dd-MM-yyyy)|\n",
      "+------------------------------+\n",
      "|                    01-01-2021|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.select(\n",
    "    date_format(col('date1'), 'dd-MM-yyyy')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cálculos con fechas y horas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('./data/calculo/calculo.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+-------------------+\n",
      "|nombre|fecha_ingreso|fecha_salida|       baja_sistema|\n",
      "+------+-------------+------------+-------------------+\n",
      "|  Jose|   2021-01-01|  2021-11-14|2021-10-14 15:35:59|\n",
      "|Mayara|   2021-02-06|  2021-11-25|2021-11-25 10:35:55|\n",
      "+------+-------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, last_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+--------------+\n",
      "|nombre|dias|      meses|ultimo_dia_mes|\n",
      "+------+----+-----------+--------------+\n",
      "|  Jose| 317|10.41935484|    2021-11-30|\n",
      "|Mayara| 292| 9.61290323|    2021-11-30|\n",
      "+------+----+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('nombre'),\n",
    "    datediff(col('fecha_salida'), col('fecha_ingreso')).alias('dias'),\n",
    "    months_between(col('fecha_salida'), col('fecha_ingreso')).alias('meses'),\n",
    "    last_day(col('fecha_salida')).alias('ultimo_dia_mes')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sumar y restar fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------+-----------+\n",
      "|nombre|fecha_ingreso|mas_14_dias|menos_1_dia|\n",
      "+------+-------------+-----------+-----------+\n",
      "|  Jose|   2021-01-01| 2021-01-15| 2020-12-31|\n",
      "|Mayara|   2021-02-06| 2021-02-20| 2021-02-05|\n",
      "+------+-------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('nombre'),\n",
    "    col('fecha_ingreso'),\n",
    "    date_add(col('fecha_ingreso'), 14).alias('mas_14_dias'),\n",
    "    date_sub(col('fecha_ingreso'), 1).alias('menos_1_dia')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extraer valores específicos de una columna date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, dayofyear, hour, minute, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|       baja_sistema|year(baja_sistema)|month(baja_sistema)|dayofmonth(baja_sistema)|dayofyear(baja_sistema)|hour(baja_sistema)|minute(baja_sistema)|second(baja_sistema)|\n",
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "|2021-10-14 15:35:59|              2021|                 10|                      14|                    287|                15|                  35|                  59|\n",
      "|2021-11-25 10:35:55|              2021|                 11|                      25|                    329|                10|                  35|                  55|\n",
      "+-------------------+------------------+-------------------+------------------------+-----------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    col('baja_sistema'),\n",
    "    year(col('baja_sistema')),\n",
    "    month(col('baja_sistema')),\n",
    "    dayofmonth(col('baja_sistema')),\n",
    "    dayofyear(col('baja_sistema')),\n",
    "    hour(col('baja_sistema')),\n",
    "    minute(col('baja_sistema')),\n",
    "    second(col('baja_sistema'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones para trabajo con strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('./data/data/data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformaciones string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| nombre|\n",
      "+-------+\n",
      "| Spark |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tiene espacios en blanco al principio y al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ltrim, rtrim, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+\n",
      "| ltrim| rtrim| trim|\n",
      "+------+------+-----+\n",
      "|Spark | Spark|Spark|\n",
      "+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    ltrim(col('nombre')).alias('ltrim'), # Elimina los espacios a la izquierda.\n",
    "    rtrim(col('nombre')).alias('rtrim'), # Elimina los espacios a la derecha.\n",
    "    trim(col('nombre')).alias('trim') # Elimina los espacios a la izquierda y derecha.\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rellenar string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lpad, rpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    lpad|    rpad|\n",
      "+--------+--------+\n",
      "|---Spark|Spark===|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    trim(col('nombre')).alias('trim'),\n",
    ").select(\n",
    "    lpad(col('trim'), 8, '-').alias('lpad'), # Agrega guiones a la izquierda.\n",
    "    rpad(col('trim'), 8, '=').alias('rpad') # Agrega igual al final.\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([('Spark', 'es', 'maravilloso')], ['sujeto', 'verbo', 'adjetivo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----------+\n",
      "|sujeto|verbo|   adjetivo|\n",
      "+------+-----+-----------+\n",
      "| Spark|   es|maravilloso|\n",
      "+------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concatenación, mayúsculas, minúsculas y reverso de string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, lower, upper, initcap, reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               frase|           minuscula|           mayuscula|             initcap|             reversa|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Spark es maravilloso|spark es maravilloso|SPARK ES MARAVILLOSO|Spark Es Maravilloso|osollivaram se krapS|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\n",
    "    concat_ws(' ', col('sujeto'), col('verbo'), col('adjetivo')).alias('frase')\n",
    ").select(\n",
    "    col('frase'),\n",
    "    lower(col('frase')).alias('minuscula'),\n",
    "    upper(col('frase')).alias('mayuscula'),\n",
    "    initcap(col('frase')).alias('initcap'),\n",
    "    reverse(col('frase')).alias('reversa')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame([(' voy a casa por mis llaves',)], ['frase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|frase                     |\n",
      "+--------------------------+\n",
      "| voy a casa por mis llaves|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|nueva_frase             |\n",
      "+------------------------+\n",
      "| ir a casa ir mis llaves|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\n",
    "    regexp_replace(col('frase'), 'voy|por', 'ir').alias('nueva_frase') # Reemplaza \"voy\" o \"por\" por ir.\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones para el trabajo con colecciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('./data/data_collections/parquet/part-00000-96f39196-ef97-4a14-926e-b24a86c2e32d-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------+\n",
      "|dia  |tareas                                      |\n",
      "+-----+--------------------------------------------+\n",
      "|lunes|[hacer la tarea, buscar agua, lavar el auto]|\n",
      "+-----+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dia: string (nullable = true)\n",
      " |-- tareas: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, size, sort_array, array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------+-----------+\n",
      "|tamaño|array_ordenado                              |buscar_agua|\n",
      "+------+--------------------------------------------+-----------+\n",
      "|3     |[buscar agua, hacer la tarea, lavar el auto]|true       |\n",
      "+------+--------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    size(col('tareas')).alias('tamaño'),\n",
    "    sort_array(col('tareas')).alias('array_ordenado'),\n",
    "    array_contains(col('tareas'), 'buscar agua').alias('buscar_agua')\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|dia  |tareas        |\n",
      "+-----+--------------+\n",
      "|lunes|hacer la tarea|\n",
      "|lunes|buscar agua   |\n",
      "|lunes|lavar el auto |\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    col('dia'),\n",
    "    explode(col('tareas')).alias('tareas')\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Formato JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_str = spark.read.parquet('./data/data_collections/JSON/part-00000-9f0e2b16-0b50-4212-9b83-654e1b8dd137-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+\n",
      "|tareas_str                                                                 |\n",
      "+---------------------------------------------------------------------------+\n",
      "|{\"dia\": \"lunes\",\"tareas\": [\"hacer la tarea\",\"buscar agua\",\"lavar el auto\"]}|\n",
      "+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df_str.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tareas_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df_str.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convertimos en estructura Spark, para ello debemos describir su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_json = StructType(\n",
    "    [\n",
    "        StructField('dia', StringType(), True),\n",
    "        StructField('tareas', ArrayType(StringType()), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = json_df_str.select(\n",
    "    from_json(col('tareas_str'), schema_json).alias('por_hacer')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- por_hacer: struct (nullable = true)\n",
      " |    |-- dia: string (nullable = true)\n",
      " |    |-- tareas: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------------------------------+--------------+\n",
      "|por_hacer.dia|por_hacer.tareas                            |primer_tarea  |\n",
      "+-------------+--------------------------------------------+--------------+\n",
      "|lunes        |[hacer la tarea, buscar agua, lavar el auto]|hacer la tarea|\n",
      "+-------------+--------------------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(\n",
    "    col('por_hacer').getItem('dia'),\n",
    "    col('por_hacer').getItem('tareas'),\n",
    "    col('por_hacer').getItem('tareas').getItem(0).alias('primer_tarea')\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|to_json(por_hacer)                                                       |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"dia\":\"lunes\",\"tareas\":[\"hacer la tarea\",\"buscar agua\",\"lavar el auto\"]}|\n",
      "+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(\n",
    "    to_json(col('por_hacer'))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones when, coalesce y lit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('./data/data/when_coalesce_lit.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|nombre|pago|\n",
      "+------+----+\n",
      "|  Jose|   1|\n",
      "| Julia|   2|\n",
      "| Katia|   1|\n",
      "|  null|   3|\n",
      "|  Raul|   3|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|nombre|       pago|\n",
      "+------+-----------+\n",
      "|  Jose|     pagado|\n",
      "| Julia|  sin pagar|\n",
      "| Katia|     pagado|\n",
      "|  null|sin iniciar|\n",
      "|  Raul|sin iniciar|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    col('nombre'),\n",
    "    when(col('pago') == 1, 'pagado').when(col('pago') == 2, 'sin pagar').otherwise('sin iniciar').alias('pago')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Con coalesce toma uno o más valores de columna y devuelve el primero que no es nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|    nombre|\n",
      "+----------+\n",
      "|      Jose|\n",
      "|     Julia|\n",
      "|     Katia|\n",
      "|sin nombre|\n",
      "|      Raul|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\n",
    "    coalesce(col('nombre'), lit('sin nombre')).alias('nombre') \n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones definidas por el usuario (UDF)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
