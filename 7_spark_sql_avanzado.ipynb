{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['JAVA_HOME'] = \"C:/Program Files/Java/jdk-11\"\n",
    "os.environ['PYSPARK_PYTHON'] = \"C:/Users/usr/anaconda3/envs/pyspark_env/python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"C:/Users/usr/anaconda3/envs/pyspark_env/python.exe\"\n",
    "os.environ['HADOOP_HOME'] = \"C:/hadoop-3.4.0\"\n",
    "os.environ['HADOOP_COMMON_LIB_NATIVE_DIR'] = \"C:/hadoop-3.4.0/lib/native\"\n",
    "os.environ['PATH'] += os.pathsep + \"C:/hadoop-3.4.0/bin\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('./data/data/vuelos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|YEAR|MONTH|DAY|DAY_OF_WEEK|AIRLINE|FLIGHT_NUMBER|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT|SCHEDULED_DEPARTURE|DEPARTURE_TIME|DEPARTURE_DELAY|TAXI_OUT|WHEELS_OFF|SCHEDULED_TIME|ELAPSED_TIME|AIR_TIME|DISTANCE|WHEELS_ON|TAXI_IN|SCHEDULED_ARRIVAL|ARRIVAL_TIME|ARRIVAL_DELAY|DIVERTED|CANCELLED|CANCELLATION_REASON|AIR_SYSTEM_DELAY|SECURITY_DELAY|AIRLINE_DELAY|LATE_AIRCRAFT_DELAY|WEATHER_DELAY|\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "|2015|1    |1  |4          |AS     |98           |N407AS     |ANC           |SEA                |5                  |2354          |-11            |21      |15        |205           |194         |169     |1448    |404      |4      |430              |408         |-22          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |AA     |2336         |N3KUAA     |LAX           |PBI                |10                 |2             |-8             |12      |14        |280           |279         |263     |2330    |737      |4      |750              |741         |-9           |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |US     |840          |N171US     |SFO           |CLT                |20                 |18            |-2             |16      |34        |286           |293         |266     |2296    |800      |11     |806              |811         |5            |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |AA     |258          |N3HYAA     |LAX           |MIA                |20                 |15            |-5             |15      |30        |285           |281         |258     |2342    |748      |8      |805              |756         |-9           |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |AS     |135          |N527AS     |SEA           |ANC                |25                 |24            |-1             |11      |35        |235           |215         |199     |1448    |254      |5      |320              |259         |-21          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |DL     |806          |N3730B     |SFO           |MSP                |25                 |20            |-5             |18      |38        |217           |230         |206     |1589    |604      |6      |602              |610         |8            |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |NK     |612          |N635NK     |LAS           |MSP                |25                 |19            |-6             |11      |30        |181           |170         |154     |1299    |504      |5      |526              |509         |-17          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |US     |2013         |N584UW     |LAX           |CLT                |30                 |44            |14             |13      |57        |273           |249         |228     |2125    |745      |8      |803              |753         |-10          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |AA     |1112         |N3LAAA     |SFO           |DFW                |30                 |19            |-11            |17      |36        |195           |193         |173     |1464    |529      |3      |545              |532         |-13          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |DL     |1173         |N826DN     |LAS           |ATL                |30                 |33            |3              |12      |45        |221           |203         |186     |1747    |651      |5      |711              |656         |-15          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |DL     |2336         |N958DN     |DEN           |ATL                |30                 |24            |-6             |12      |36        |173           |149         |133     |1199    |449      |4      |523              |453         |-30          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |AA     |1674         |N853AA     |LAS           |MIA                |35                 |27            |-8             |21      |48        |268           |266         |238     |2174    |746      |7      |803              |753         |-10          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |DL     |1434         |N547US     |LAX           |MSP                |35                 |35            |0              |18      |53        |214           |210         |188     |1535    |601      |4      |609              |605         |-4           |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |DL     |2324         |N3751B     |SLC           |ATL                |40                 |34            |-6             |18      |52        |215           |199         |176     |1590    |548      |5      |615              |553         |-22          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |DL     |2440         |N651DL     |SEA           |MSP                |40                 |39            |-1             |28      |107       |189           |198         |166     |1399    |553      |4      |549              |557         |8            |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |AS     |108          |N309AS     |ANC           |SEA                |45                 |41            |-4             |17      |58        |204           |194         |173     |1448    |451      |4      |509              |455         |-14          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |DL     |1560         |N3743H     |ANC           |SEA                |45                 |31            |-14            |25      |56        |210           |200         |171     |1448    |447      |4      |515              |451         |-24          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |UA     |1197         |N78448     |SFO           |IAH                |48                 |42            |-6             |11      |53        |218           |217         |199     |1635    |612      |7      |626              |619         |-7           |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |AS     |122          |N413AS     |ANC           |PDX                |50                 |46            |-4             |11      |57        |215           |201         |187     |1542    |504      |3      |525              |507         |-18          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "|2015|1    |1  |4          |DL     |1670         |N806DN     |PDX           |MSP                |50                 |45            |-5             |9       |54        |193           |186         |171     |1426    |545      |6      |603              |551         |-12          |0       |0        |null               |null            |null          |null         |null               |null         |\n",
      "+----+-----+---+-----------+-------+-------------+-----------+--------------+-------------------+-------------------+--------------+---------------+--------+----------+--------------+------------+--------+--------+---------+-------+-----------------+------------+-------------+--------+---------+-------------------+----------------+--------------+-------------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones count(), countDistinct() y approx_count_distinct()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.parquet('./data/data/dataframe.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- cantidad: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+\n",
      "|nombre|color|cantidad|\n",
      "+------+-----+--------+\n",
      "|  Jose| azul|    1900|\n",
      "|  null| null|    1700|\n",
      "|  null| rojo|    1300|\n",
      "|  Juan| rojo|    1500|\n",
      "+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **count()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cuenta aquellos que no son nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|conteo_nombre|conteo_color|\n",
      "+-------------+------------+\n",
      "|            2|           3|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\n",
    "    count('nombre').alias('conteo_nombre'),\n",
    "    count('color').alias('conteo_color')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- count('*') cuenta todas las filas, aunque haya al algún null en ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------------+\n",
      "|conteo_nombre|conteo_color|conteo_general|\n",
      "+-------------+------------+--------------+\n",
      "|            2|           3|             4|\n",
      "+-------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\n",
    "    count('nombre').alias('conteo_nombre'),\n",
    "    count('color').alias('conteo_color'),\n",
    "    count('*'). alias('conteo_general')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **countDistinct()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|colores_dif|\n",
      "+-----------+\n",
      "|          2|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\n",
    "    countDistinct('color').alias('colores_dif')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **approx_count_distinct()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark.read.parquet('./data/data/vuelos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En una base de datos grande hacer un count() puede ser costoso, nos bastará muchas veces con un conteo aproximado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------------------+\n",
      "|count(DISTINCT AIRLINE)|approx_count_distinct(AIRLINE)|\n",
      "+-----------------------+------------------------------+\n",
      "|                     14|                            13|\n",
      "+-----------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.select(\n",
    "    countDistinct('AIRLINE'),\n",
    "    approx_count_distinct('AIRLINE')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Funciones min() y max()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vuelos = spark.read.parquet('./data/data/vuelos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vuelos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|menor_tiempo|mayor_tiempo|\n",
      "+------------+------------+\n",
      "|           7|         690|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vuelos.select(\n",
    "    min('AIR_TIME').alias('menor_tiempo'),\n",
    "    max('AIR_TIME').alias('mayor_tiempo')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|min(AIRLINE_DELAY)|max(AIRLINE_DELAY)|\n",
      "+------------------+------------------+\n",
      "|                 0|              1971|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vuelos.select(\n",
    "    min('AIRLINE_DELAY'),\n",
    "    max('AIRLINE_DELAY')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **sum(), sumDistinct() and avg() Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vuelos = spark.read.parquet('./data/data/vuelos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, sumDistinct, avg, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **sum()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vuelos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   sum_dis|\n",
      "+----------+\n",
      "|4785357409|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vuelos.select(\n",
    "    sum('DISTANCE').alias('sum_dis')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **sumDistinct()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does sum() but only with the different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\usr\\anaconda3\\envs\\pyspark_env\\lib\\site-packages\\pyspark\\sql\\functions.py:752: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum_dis_dif|\n",
      "+-----------+\n",
      "|    1442300|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vuelos.select(\n",
    "    sumDistinct('DISTANCE').alias('sum_dis_dif')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **avg()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|     promedio_aire|       prom_manual|\n",
      "+------------------+------------------+\n",
      "|113.51162809012519|113.51162809012519|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vuelos.select(\n",
    "    avg('AIR_TIME').alias('promedio_aire'),\n",
    "    (sum('AIR_TIME') / count('AIR_TIME')).alias('prom_manual')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Aggregation and Grouping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Aggregation** is generally used with low cardinality categorical variables (e.g.: sex, gender, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It has 2 steps, first we use groupBy with the columns we want to group, there is where we specify the columns we want to group the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second step is to aggregate the aggregation functions we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vuelos = spark.read.parquet('./data/data/vuelos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vuelos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want to take the DF, group by 'ORIGIN_AIRPORT' and count how many flights there are from the 'ORIGIN_AIRPORT'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|ORIGIN_AIRPORT| count|\n",
      "+--------------+------+\n",
      "|           ATL|346836|\n",
      "|           ORD|285884|\n",
      "|           DFW|239551|\n",
      "|           DEN|196055|\n",
      "|           LAX|194673|\n",
      "|           SFO|148008|\n",
      "|           PHX|146815|\n",
      "|           IAH|146622|\n",
      "|           LAS|133181|\n",
      "|           MSP|112117|\n",
      "|           MCO|110982|\n",
      "|           SEA|110899|\n",
      "|           DTW|108500|\n",
      "|           BOS|107847|\n",
      "|           EWR|101772|\n",
      "|           CLT|100324|\n",
      "|           LGA| 99605|\n",
      "|           SLC| 97210|\n",
      "|           JFK| 93811|\n",
      "|           BWI| 86079|\n",
      "+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(vuelos.groupBy('ORIGIN_AIRPORT')\n",
    "    .count()\n",
    "    .orderBy(desc('count'))\n",
    " ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we want to group by different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-----+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|count|\n",
      "+--------------+-------------------+-----+\n",
      "|           SFO|                LAX|13744|\n",
      "|           LAX|                SFO|13457|\n",
      "|           JFK|                LAX|12016|\n",
      "|           LAX|                JFK|12015|\n",
      "|           LAS|                LAX| 9715|\n",
      "|           LGA|                ORD| 9639|\n",
      "|           LAX|                LAS| 9594|\n",
      "|           ORD|                LGA| 9575|\n",
      "|           SFO|                JFK| 8440|\n",
      "|           JFK|                SFO| 8437|\n",
      "|           OGG|                HNL| 8313|\n",
      "|           HNL|                OGG| 8282|\n",
      "|           LAX|                ORD| 8256|\n",
      "|           ATL|                LGA| 8234|\n",
      "|           LGA|                ATL| 8215|\n",
      "|           ATL|                MCO| 8202|\n",
      "|           MCO|                ATL| 8202|\n",
      "|           SFO|                LAS| 7995|\n",
      "|           ORD|                LAX| 7941|\n",
      "|           LAS|                SFO| 7870|\n",
      "+--------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(vuelos.groupBy('ORIGIN_AIRPORT', 'DESTINATION_AIRPORT')\n",
    "    .count()\n",
    "    .orderBy(desc('count'))\n",
    " ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Several aggregations per group()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vuelos = spark.read.parquet('./data/data/vuelos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, min, max, desc, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---+---+\n",
      "|ORIGIN_AIRPORT|tiempo_aire|min|max|\n",
      "+--------------+-----------+---+---+\n",
      "|           ATL|     343506| 15|614|\n",
      "|           ORD|     276554| 13|571|\n",
      "|           DFW|     232647| 11|534|\n",
      "|           DEN|     193402| 12|493|\n",
      "|           LAX|     192003| 14|409|\n",
      "|           PHX|     145552| 19|444|\n",
      "|           SFO|     145491|  8|389|\n",
      "|           IAH|     144019| 15|524|\n",
      "|           LAS|     131937| 25|429|\n",
      "|           MSP|     111055| 14|537|\n",
      "|           SEA|     110178| 17|412|\n",
      "|           MCO|     109532| 25|395|\n",
      "|           DTW|     106992| 15|341|\n",
      "|           BOS|     104804| 16|432|\n",
      "|           CLT|      99052| 17|379|\n",
      "|           EWR|      98341| 21|683|\n",
      "|           SLC|      96505| 18|419|\n",
      "|           LGA|      94834| 19|311|\n",
      "|           JFK|      91663| 29|690|\n",
      "|           BWI|      84329| 19|398|\n",
      "+--------------+-----------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vuelos.groupBy('ORIGIN_AIRPORT').agg(\n",
    "    count('AIR_TIME').alias('tiempo_aire'),\n",
    "    min('AIR_TIME').alias('min'),\n",
    "    max('AIR_TIME').alias('max')\n",
    ").orderBy(desc('tiempo_aire')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have the rows from the AIR_TIME column of each airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------------+\n",
      "|MONTH|conteo_de_retrasos|        prom_dist|\n",
      "+-----+------------------+-----------------+\n",
      "|    7|            514384|841.4772794487611|\n",
      "|    8|            503956|834.8244276603413|\n",
      "|    6|            492847|835.6302716626612|\n",
      "|    3|            492138|816.0553268611494|\n",
      "|    5|            489641|823.3230588760807|\n",
      "|   10|            482878|816.4436127652134|\n",
      "|    4|            479251|817.0060476016745|\n",
      "|   12|            469717|837.8018926194103|\n",
      "|   11|            462367|820.2482434846529|\n",
      "|    9|            462153|815.8487523282274|\n",
      "|    1|            457013|803.2612794913696|\n",
      "|    2|            407663| 800.785449834689|\n",
      "+-----+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vuelos.groupBy('MONTH').agg(\n",
    "    count('ARRIVAL_DELAY').alias('conteo_de_retrasos'),\n",
    "    avg('DISTANCE').alias('prom_dist')\n",
    ").orderBy(desc('conteo_de_retrasos')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we group by MONTH and we count how many flights were delayed, we order it by descending and do the average of the distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Aggregate and Pivot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "estudiantes = spark.read.parquet('./data/data/estudiantes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+----------+\n",
      "|nombre|sexo|peso|graduacion|\n",
      "+------+----+----+----------+\n",
      "|  Jose|   M|  80|      2000|\n",
      "| Hilda|   F|  50|      2000|\n",
      "|  Juan|   M|  75|      2000|\n",
      "| Pedro|   M|  76|      2001|\n",
      "|Katia+|   F|  65|      2001|\n",
      "+------+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estudiantes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Promedio de peso por año de graduación y sexo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, avg, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+\n",
      "|graduacion|   F|   M|\n",
      "+----------+----+----+\n",
      "|      2001|65.0|76.0|\n",
      "|      2000|50.0|77.5|\n",
      "+----------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|graduacion|F_avg(peso)|F_min(peso)|F_max(peso)|M_avg(peso)|M_min(peso)|M_max(peso)|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|      2001|       65.0|         65|         65|       76.0|         76|         76|\n",
      "|      2000|       50.0|         50|         50|       77.5|         75|         80|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso'), min('peso'), max('peso')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos indicar que se muestren solo determinados valores a través de una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+\n",
      "|graduacion|M_avg(peso)|M_min(peso)|M_max(peso)|\n",
      "+----------+-----------+-----------+-----------+\n",
      "|      2001|       76.0|         76|         76|\n",
      "|      2000|       77.5|         75|         80|\n",
      "+----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estudiantes.groupBy('graduacion').pivot('sexo', ['M']).agg(avg('peso'), min('peso'), max('peso')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+\n",
      "|graduacion|F_avg(peso)|F_min(peso)|F_max(peso)|\n",
      "+----------+-----------+-----------+-----------+\n",
      "|      2001|       65.0|         65|         65|\n",
      "|      2000|       50.0|         50|         50|\n",
      "+----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estudiantes.groupBy('graduacion').pivot('sexo', ['F']).agg(avg('peso'), min('peso'), max('peso')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inner Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "empleados = spark.read.parquet('./data/empleados/empleados.parquet')\n",
    "departamentos = spark.read.parquet('./data/departamentos/departamentos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|nombre|num_dpto|\n",
      "+------+--------+\n",
      "|  Luis|      33|\n",
      "| Katia|      33|\n",
      "|  Raul|      34|\n",
      "| Pedro|       0|\n",
      "| Laura|      34|\n",
      "|Sandro|      31|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|nombre_dpto|\n",
      "+---+-----------+\n",
      "| 31|     letras|\n",
      "| 33|    derecho|\n",
      "| 34| matemática|\n",
      "| 35|informática|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departamentos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es el más utilizado y es el predeterminado (opcional especificarlo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Especificamos dentro de .join la condición de cruce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Raul|      34| 34| matemática|\n",
      "| Laura|      34| 34| matemática|\n",
      "|Sandro|      31| 31|     letras|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = empleados.join(departamentos, col('num_dpto') == col('id'))\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos hacer la misma consulta, especificamos el tipo de join después de realizar la consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Raul|      34| 34| matemática|\n",
      "| Laura|      34| 34| matemática|\n",
      "|Sandro|      31| 31|     letras|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = empleados.join(departamentos, col('num_dpto') == col('id'), 'inner')\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otra forma de redactar lo mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Raul|      34| 34| matemática|\n",
      "| Laura|      34| 34| matemática|\n",
      "|Sandro|      31| 31|     letras|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = empleados.join(departamentos).where(col('num_dpto') == col('id'))\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Left Outer Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+-----------+\n",
      "|nombre|num_dpto|  id|nombre_dpto|\n",
      "+------+--------+----+-----------+\n",
      "|  Luis|      33|  33|    derecho|\n",
      "| Katia|      33|  33|    derecho|\n",
      "|  Raul|      34|  34| matemática|\n",
      "| Pedro|       0|null|       null|\n",
      "| Laura|      34|  34| matemática|\n",
      "|Sandro|      31|  31|     letras|\n",
      "+------+--------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.join(departamentos, col('num_dpto') == col('id'), 'leftouter').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Contiene todos los elementos del df de la izquierda (empleados) y rellena con nulos los elementos de df de la derecha donde no se hizo cruce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otra forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+-----------+\n",
      "|nombre|num_dpto|  id|nombre_dpto|\n",
      "+------+--------+----+-----------+\n",
      "|  Luis|      33|  33|    derecho|\n",
      "| Katia|      33|  33|    derecho|\n",
      "|  Raul|      34|  34| matemática|\n",
      "| Pedro|       0|null|       null|\n",
      "| Laura|      34|  34| matemática|\n",
      "|Sandro|      31|  31|     letras|\n",
      "+------+--------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.join(departamentos, col('num_dpto') == col('id'), 'left_outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otra forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+-----------+\n",
      "|nombre|num_dpto|  id|nombre_dpto|\n",
      "+------+--------+----+-----------+\n",
      "|  Luis|      33|  33|    derecho|\n",
      "| Katia|      33|  33|    derecho|\n",
      "|  Raul|      34|  34| matemática|\n",
      "| Pedro|       0|null|       null|\n",
      "| Laura|      34|  34| matemática|\n",
      "|Sandro|      31|  31|     letras|\n",
      "+------+--------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.join(departamentos, col('num_dpto') == col('id'), 'left').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Right Outer Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|Sandro|      31| 31|     letras|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Laura|      34| 34| matemática|\n",
      "|  Raul|      34| 34| matemática|\n",
      "|  null|    null| 35|informática|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.join(departamentos, col('num_dpto') == col('id'), 'rightouter').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es el inverso al left outer, mantiene todos los datos del df de la derecha y rellena con nulos los que no hicieron cruce del df de la izquierda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otra forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|Sandro|      31| 31|     letras|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Laura|      34| 34| matemática|\n",
      "|  Raul|      34| 34| matemática|\n",
      "|  null|    null| 35|informática|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.join(departamentos, col('num_dpto') == col('id'), 'right_outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otra forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|Sandro|      31| 31|     letras|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Laura|      34| 34| matemática|\n",
      "|  Raul|      34| 34| matemática|\n",
      "|  null|    null| 35|informática|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.join(departamentos, col('num_dpto') == col('id'), 'right').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Full Outer Join**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+-----------+\n",
      "|nombre|num_dpto|  id|nombre_dpto|\n",
      "+------+--------+----+-----------+\n",
      "| Pedro|       0|null|       null|\n",
      "|Sandro|      31|  31|     letras|\n",
      "|  Luis|      33|  33|    derecho|\n",
      "| Katia|      33|  33|    derecho|\n",
      "|  Raul|      34|  34| matemática|\n",
      "| Laura|      34|  34| matemática|\n",
      "|  null|    null|  35|informática|\n",
      "+------+--------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.join(departamentos, col('num_dpto') == col('id'), 'outer').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es el mismo comportamiento que si combinaramos left outer y right outer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Left Anti Join**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Permite saber qué filas del df de la izquierda no tienen cruce con el df de la derecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|nombre|num_dpto|\n",
      "+------+--------+\n",
      "| Pedro|       0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.join(departamentos, col('num_dpto') == col('id'), 'left_anti').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|nombre_dpto|\n",
      "+---+-----------+\n",
      "| 35|informática|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departamentos.join(empleados, col('num_dpto') == col('id'), 'left_anti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Left Semi Join**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similar a inner join pero los datos resultantes no incluyen el conjunto de columnas de la derecha. Es el opuesto al left anti join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|nombre|num_dpto|\n",
      "+------+--------+\n",
      "|  Luis|      33|\n",
      "| Katia|      33|\n",
      "|  Raul|      34|\n",
      "| Laura|      34|\n",
      "|Sandro|      31|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.join(departamentos, col('num_dpto') == col('id'), 'left_semi').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El único registro que no devuelve es el que no se encuentra en el df de la derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross Join**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La expresión de join no es necesaria, multiplica el número de filas del df1 por el número de filas del df2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|  Luis|      33| 31|     letras|\n",
      "|  Luis|      33| 33|    derecho|\n",
      "|  Luis|      33| 34| matemática|\n",
      "|  Luis|      33| 35|informática|\n",
      "| Katia|      33| 31|     letras|\n",
      "| Katia|      33| 33|    derecho|\n",
      "| Katia|      33| 34| matemática|\n",
      "| Katia|      33| 35|informática|\n",
      "|  Raul|      34| 31|     letras|\n",
      "|  Raul|      34| 33|    derecho|\n",
      "|  Raul|      34| 34| matemática|\n",
      "|  Raul|      34| 35|informática|\n",
      "| Pedro|       0| 31|     letras|\n",
      "| Pedro|       0| 33|    derecho|\n",
      "| Pedro|       0| 34| matemática|\n",
      "| Pedro|       0| 35|informática|\n",
      "| Laura|      34| 31|     letras|\n",
      "| Laura|      34| 33|    derecho|\n",
      "| Laura|      34| 34| matemática|\n",
      "| Laura|      34| 35|informática|\n",
      "+------+--------+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = empleados.crossJoin(departamentos)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Manejo de nombres de columnas duplicados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agregamos una nueva columna al df que será igual a la columna \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "depa = departamentos.withColumn('num_dpto', col('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre_dpto: string (nullable = true)\n",
      " |-- num_dpto: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- num_dpto: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "depa.printSchema()\n",
    "empleados.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empleados.join(depa, col('num_dpto') == col('num_dpto')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Esto nos da error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lo podemos solucionar informando a Spark de qué df queremos que tome la columna ambigua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con_duplicados = empleados.join(depa, empleados['num_dpto'] == depa['num_dpto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+--------+\n",
      "|nombre|num_dpto| id|nombre_dpto|num_dpto|\n",
      "+------+--------+---+-----------+--------+\n",
      "|Sandro|      31| 31|     letras|      31|\n",
      "| Katia|      33| 33|    derecho|      33|\n",
      "|  Luis|      33| 33|    derecho|      33|\n",
      "| Laura|      34| 34| matemática|      34|\n",
      "|  Raul|      34| 34| matemática|      34|\n",
      "+------+--------+---+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_con_duplicados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- num_dpto: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre_dpto: string (nullable = true)\n",
      " |-- num_dpto: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_con_duplicados.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿Cómo solucionamos el problema de columnas duplicadas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con_duplicados.select('num_dpto').show() # Nos dará error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark recuerda de qué df vino cada columna en el join, se le puede pedir a Spark que indique el prefijo de su df original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|num_dpto|\n",
      "+--------+\n",
      "|      33|\n",
      "|      33|\n",
      "|      34|\n",
      "|      34|\n",
      "|      31|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_con_duplicados.select(empleados['num_dpto']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otra opción, la más recomendada, es usar una tercera columna de unión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = empleados.join(depa, 'num_dpto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num_dpto: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre_dpto: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema() # Ya no la duplica, la interpreta como una sola columna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otra opción muy similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+-----------+\n",
      "|num_dpto|nombre| id|nombre_dpto|\n",
      "+--------+------+---+-----------+\n",
      "|      31|Sandro| 31|     letras|\n",
      "|      33| Katia| 33|    derecho|\n",
      "|      33|  Luis| 33|    derecho|\n",
      "|      34| Laura| 34| matemática|\n",
      "|      34|  Raul| 34| matemática|\n",
      "+--------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = empleados.join(depa, ['num_dpto']).show() # A través de una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num_dpto: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre_dpto: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.join(depa, ['num_dpto']).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Shuffle Hash Join y Broadcast Hash Join**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark utiliza shuffle cuando ambos conjuntos de datos son grandes, broadcast cuando uno de los conjuntos de datos es pequeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+\n",
      "|nombre|num_dpto| id|nombre_dpto|\n",
      "+------+--------+---+-----------+\n",
      "|  Luis|      33| 33|    derecho|\n",
      "| Katia|      33| 33|    derecho|\n",
      "|  Raul|      34| 34| matemática|\n",
      "| Laura|      34| 34| matemática|\n",
      "|Sandro|      31| 31|     letras|\n",
      "+------+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.join(broadcast(departamentos), col('num_dpto') == col('id')).show() # df departamentos debe ser más pequeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comprobamos que Spark ha hecho un broadcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [num_dpto#4697L], [id#4700L], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(num_dpto#4697L)\n",
      "   :  +- FileScan parquet [nombre#4696,num_dpto#4697L] Batched: true, DataFilters: [isnotnull(num_dpto#4697L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/usr/Documents/GitHub/lab-pyspark/data/empleados/emplead..., PartitionFilters: [], PushedFilters: [IsNotNull(num_dpto)], ReadSchema: struct<nombre:string,num_dpto:bigint>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=3173]\n",
      "      +- Filter isnotnull(id#4700L)\n",
      "         +- FileScan parquet [id#4700L,nombre_dpto#4701] Batched: true, DataFilters: [isnotnull(id#4700L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/usr/Documents/GitHub/lab-pyspark/data/departamentos/dep..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint,nombre_dpto:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empleados.join(broadcast(departamentos), col('num_dpto') == col('id')).explain() # Hace un BroadcastHashJoin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
